{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbb65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dd2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, \\\n",
    "                            accuracy_score, auc, precision_recall_curve, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091d364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c875acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 20\n",
    "\n",
    "start_date = datetime.datetime(2023, 4, 3)\n",
    "\n",
    "start_dates = [start_date + relativedelta(days = i) for i in range(n_days + 1)]\n",
    "start_dates = [i.date().strftime('%Y_%m_%d') for i in start_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a3b25702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()  \n",
    "pair = 'BTCEUR'\n",
    "\n",
    "freq = 1\n",
    "\n",
    "tc = 'received_time_r'\n",
    "\n",
    "for date in start_dates:\n",
    "    \n",
    "    lt_df = pd.read_csv(f'~/AST_TRADE_{pair}_{date}_{freq}.csv').rename(columns = {'event_time_r': 'received_time_r'})\n",
    "\n",
    "    lt_df.columns = [tc] + [i + '_LT' for i in lt_df.columns[1:]]\n",
    "\n",
    "    df = df.append(lt_df)\n",
    "\n",
    "extremes = dict()\n",
    "\n",
    "for c in df.columns[1:-1]:\n",
    "    if df[c].max() == np.inf or df[c].max() > np.inf:\n",
    "        extremes[c] = 1\n",
    "    elif df[c].min() == -np.inf or df[c].min() < -np.inf:\n",
    "        extremes[c] = 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for c in extremes.keys():\n",
    "    df[c] = np.where(df[c] >= np.inf, df[c][df[c] < np.inf].quantile(0.9), df[c])\n",
    "    df[c] = np.where(df[c] <= -np.inf, df[c][df[c] > -np.inf].quantile(0.1), df[c])\n",
    "    \n",
    "\n",
    "no_variation = dict()\n",
    "\n",
    "for c in df.columns[1:-1]:\n",
    "    if np.nanstd(df[c]) == 0:\n",
    "        no_variation[c] = 1\n",
    "\n",
    "df.drop(columns = no_variation.keys(), inplace = True) \n",
    "\n",
    "df['minute'] = df['received_time_r'].apply(lambda x: x[-5:-3])\n",
    "df['second'] = df['received_time_r'].apply(lambda x: x[-2:])\n",
    "\n",
    "minutes_df = pd.get_dummies(df['minute'], prefix = 'minute', drop_first = True).astype(int)\n",
    "seconds_df = pd.get_dummies(df['second'], prefix = 'second', drop_first = True).astype(int)\n",
    "\n",
    "df.drop(columns = ['minute', 'second'], inplace = True)\n",
    "\n",
    "df = pd.concat([df, minutes_df], axis = 1)\n",
    "df = pd.concat([df, seconds_df], axis = 1)\n",
    "\n",
    "del minutes_df\n",
    "del seconds_df\n",
    "\n",
    "df['received_time_r'] = df['received_time_r'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067d6920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return np.nanmean(abs(y_true - y_pred))\n",
    "\n",
    "def rmae(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / np.nanmean(y_true)\n",
    "\n",
    "def sliding_window_cv(data, target, min_sample, timestamp_col, window, algorythm, algo_params, freq, \n",
    "                      pair, scaling = False):\n",
    "    \n",
    "    date_col = 'date_col'\n",
    "    \n",
    "    data[date_col] = data[timestamp_col].apply(lambda x: x.date())\n",
    "    start_date, end_date = data[date_col].min(), data[date_col].max()\n",
    "    \n",
    "    delta = (end_date - start_date).days\n",
    "    n_iter = delta - window - min_sample + 1\n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    l = int(data.shape[0] * 0.1)\n",
    "    \n",
    "    for i in tqdm.tqdm_notebook(range(n_iter + 1)):\n",
    "        delta = relativedelta(days = +i)\n",
    "        start, end = start_date + delta, start_date + delta + relativedelta(days = +min_sample)\n",
    "        \n",
    "        train = data[(data[date_col] >= start) & (data[date_col] < end)]\n",
    "        test = data[(data[date_col] >= end) & (data[date_col] < end + relativedelta(days = +window))]\n",
    "        \n",
    "        print(train[date_col].min(), train[date_col].max(), test[date_col].unique())\n",
    "        \n",
    "        x_train, y_train = train.drop(columns = [target, timestamp_col, date_col]), train[target]\n",
    "        x_test, y_test = test.drop(columns = [target, timestamp_col, date_col]), test[target]\n",
    "        \n",
    "        if scaling:\n",
    "            sc = StandardScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "            x_test = sc.transform(x_test)\n",
    "            \n",
    "        model = algorythm(**algo_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        file_name = f\"xgb_{pair}_sw_cv_{datetime.datetime.strftime(start, '%Y_%m_%d')}_w{window}_freq{freq}.pkl\"\n",
    "        pickle.dump(model, open(file_name, \"wb\"))\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        threshold = np.nanquantile(y_pred[:l], (y_train == 0).mean())\n",
    "\n",
    "        y_pred_bin = np.where(y_pred[l:] < threshold, 1, 0)\n",
    "        y_test_bin = np.where(y_test[l:] == 0, 1, 0)\n",
    "\n",
    "        recall = recall_score(y_test_bin, y_pred_bin)\n",
    "        precision = precision_score(y_test_bin, y_pred_bin)\n",
    "        f1 = f1_score(y_test_bin, y_pred_bin)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_rw = r2_score(y_test[:-1], y_test[1:])\n",
    "\n",
    "        mae_model = abs(y_pred - y_test)[1:]\n",
    "        mae_rw = abs(y_test[1:] - y_test[:-1])\n",
    "        mae_avg = abs(y_test - y_train.mean())\n",
    "\n",
    "        diff_rw = mae_rw - mae_model\n",
    "        tstat_rw = diff_rw.mean() / diff_rw.std() * np.sqrt(diff_rw.shape[0] - 1)\n",
    "\n",
    "        diff_avg = mae_avg - mae_model\n",
    "        tstat_avg = diff_avg.mean() / diff_avg.std() * np.sqrt(diff_avg.shape[0] - 1)\n",
    "        \n",
    "        fi = pd.DataFrame(zip(train.drop(columns = [target, timestamp_col, date_col]).columns, \n",
    "                              model.feature_importances_), \n",
    "                          columns = ['feature', 'importance']).sort_values(by = 'importance', ascending = False)\n",
    "        \n",
    "        fi.to_excel(f'SW_FI_{pair}_freq{freq}.xlsx', index = None)\n",
    "        \n",
    "        results[end] = {'mae': mae(y_test, y_pred), 'rmae': rmae(y_test, y_pred),\n",
    "                        'recall': recall, 'precision': precision, 'f1': f1,\n",
    "                        'r2': r2, 'r2_rw': r2_rw, 'tstat_rw': tstat_rw, 'tstat_avg': tstat_avg,\n",
    "                        'avg_zero_test': y_test_bin.mean(), 'avg_zero_pred': y_pred_bin.mean(), \n",
    "                        'fi': fi}\n",
    "        \n",
    "    return results\n",
    "\n",
    "def expanding_window_cv(data, target, min_sample, timestamp_col, window, algorythm, algo_params, freq,\n",
    "                        pair, scaling = False):\n",
    "    \n",
    "    date_col = 'date_col'\n",
    "    \n",
    "    data[date_col] = data[timestamp_col].apply(lambda x: x.date())\n",
    "    start_date, end_date = data[date_col].min(), data[date_col].max()\n",
    "    \n",
    "    delta = (end_date - start_date).days\n",
    "    n_iter = delta - window - min_sample + 1\n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    l = int(data.shape[0] * 0.1)\n",
    "    \n",
    "    for i in range(n_iter + 1):\n",
    "        delta = relativedelta(days = +i)\n",
    "        start, end = start_date, start_date + delta + relativedelta(days = +min_sample)\n",
    "        \n",
    "        train = data[(data[date_col] >= start) & (data[date_col] < end)]\n",
    "        test = data[(data[date_col] >= end) & (data[date_col] < end + relativedelta(days = +window))]\n",
    "        \n",
    "        print(train[date_col].min(), train[date_col].max(), test[date_col].unique())\n",
    "        \n",
    "        x_train, y_train = train.drop(columns = [target, timestamp_col, date_col]), train[target]\n",
    "        x_test, y_test = test.drop(columns = [target, timestamp_col, date_col]), test[target]\n",
    "        \n",
    "        if scaling:\n",
    "            sc = StandardScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "            x_test = sc.transform(x_test)\n",
    "            \n",
    "        model = algorythm(**algo_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        file_name = f\"xgb_{pair}_ew_cv_{datetime.datetime.strftime(start, '%Y_%m_%d')}_w{window}_freq{freq}.pkl\"\n",
    "        pickle.dump(model, open(file_name, \"wb\"))\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        threshold = np.nanquantile(y_pred[:l], (y_train == 0).mean())\n",
    "\n",
    "        y_pred_bin = np.where(y_pred[l:] < threshold, 1, 0)\n",
    "        y_test_bin = np.where(y_test[l:] == 0, 1, 0)\n",
    "\n",
    "        recall = recall_score(y_test_bin, y_pred_bin)\n",
    "        precision = precision_score(y_test_bin, y_pred_bin)\n",
    "        f1 = f1_score(y_test_bin, y_pred_bin)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_rw = r2_score(y_test[:-1], y_test[1:])\n",
    "\n",
    "        mae_model = abs(y_pred - y_test)[1:]\n",
    "        mae_rw = abs(y_test[1:] - y_test[:-1])\n",
    "        mae_avg = abs(y_test - y_train.mean())\n",
    "\n",
    "        diff_rw = mae_rw - mae_model\n",
    "        tstat_rw = diff_rw.mean() / diff_rw.std() * np.sqrt(diff_rw.shape[0] - 1)\n",
    "\n",
    "        diff_avg = mae_avg - mae_model\n",
    "        tstat_avg = diff_avg.mean() / diff_avg.std() * np.sqrt(diff_avg.shape[0] - 1)\n",
    "        \n",
    "        fi = pd.DataFrame(zip(train.drop(columns = [target, timestamp_col, date_col]).columns, \n",
    "                              model.feature_importances_), \n",
    "                          columns = ['feature', 'importance']).sort_values(by = 'importance', ascending = False)\n",
    "        \n",
    "        fi.to_excel(f'EW_FI_{pair}_freq{freq}.xlsx', index = None)\n",
    "        \n",
    "        results[end] = {'mae': mae(y_test, y_pred), 'rmae': rmae(y_test, y_pred),\n",
    "                        'recall': recall, 'precision': precision, 'f1': f1,\n",
    "                        'r2': r2, 'r2_rw': r2_rw, 'tstat_rw': tstat_rw, 'tstat_avg': tstat_avg,\n",
    "                        'avg_zero_test': y_test_bin.mean(), 'avg_zero_pred': y_pred_bin.mean(),\n",
    "                        'fi': fi}\n",
    "        \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "7b1922b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'n_estimators': 500, \n",
    "          'max_depth': 6, \n",
    "          'learning_rate': 0.05, \n",
    "          'reg_alpha': 10, \n",
    "          'random_state': 81, \n",
    "          'n_jobs': 9\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "fffffb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efb8126e53e49fd8eacb8d8610e9cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 2023-04-09 [datetime.date(2023, 4, 10)]\n",
      "2023-04-04 2023-04-10 [datetime.date(2023, 4, 11)]\n",
      "2023-04-05 2023-04-11 [datetime.date(2023, 4, 12)]\n",
      "2023-04-06 2023-04-12 [datetime.date(2023, 4, 13)]\n",
      "2023-04-07 2023-04-13 [datetime.date(2023, 4, 14)]\n",
      "2023-04-08 2023-04-14 [datetime.date(2023, 4, 15)]\n",
      "2023-04-09 2023-04-15 [datetime.date(2023, 4, 16)]\n",
      "2023-04-10 2023-04-16 [datetime.date(2023, 4, 17)]\n",
      "2023-04-11 2023-04-17 [datetime.date(2023, 4, 18)]\n",
      "2023-04-12 2023-04-18 [datetime.date(2023, 4, 19)]\n",
      "2023-04-13 2023-04-19 [datetime.date(2023, 4, 20)]\n",
      "2023-04-14 2023-04-20 [datetime.date(2023, 4, 21)]\n",
      "2023-04-15 2023-04-21 [datetime.date(2023, 4, 22)]\n",
      "2023-04-16 2023-04-22 [datetime.date(2023, 4, 23)]\n"
     ]
    }
   ],
   "source": [
    "res_sliding = sliding_window_cv(df, 'rv_LT', 7, 'received_time_r', 1, XGBRegressor, params, freq, pair,\n",
    "                                scaling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "64bf0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_results = pd.DataFrame({'date': [i for i in res_sliding.keys()],\n",
    "                                'mae': [res_sliding[i]['mae'] for i, j in res_sliding.items()],\n",
    "                                'rmae': [res_sliding[i]['rmae'] for i, j in res_sliding.items()],\n",
    "                                'recall': [res_sliding[i]['recall'] for i, j in res_sliding.items()],\n",
    "                                'precision': [res_sliding[i]['precision'] for i, j in res_sliding.items()],\n",
    "                                'f1': [res_sliding[i]['f1'] for i, j in res_sliding.items()],\n",
    "                                'r2': [res_sliding[i]['r2'] for i, j in res_sliding.items()], \n",
    "                                'r2_rw': [res_sliding[i]['r2_rw'] for i, j in res_sliding.items()],\n",
    "                                'tstat_rw': [res_sliding[i]['tstat_rw'] for i, j in res_sliding.items()],\n",
    "                                'tstat_avg': [res_sliding[i]['tstat_avg'] for i, j in res_sliding.items()],\n",
    "                                'avg_zero_test': [res_sliding[i]['avg_zero_test'] for i, j in res_sliding.items()],\n",
    "                                'avg_zero_pred': [res_sliding[i]['avg_zero_pred'] for i, j in res_sliding.items()]} \n",
    "                               )\n",
    "\n",
    "sliding_results.to_excel(f'SW_{pair}_RESULTS_FREQ{freq}_BOOSTING.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "52ad207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_features = pd.DataFrame()\n",
    "\n",
    "for i in res_sliding:\n",
    "    tmp_df = res_sliding[i]['fi']\n",
    "    tmp_df['date'] = i\n",
    "    \n",
    "    sliding_features = sliding_features.append(tmp_df)\n",
    "    \n",
    "sliding_features.to_excel(f'SW_FI_{pair}_freq{freq}.xlsx', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "62ca9f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 2023-04-09 [datetime.date(2023, 4, 10)]\n",
      "2023-04-03 2023-04-10 [datetime.date(2023, 4, 11)]\n",
      "2023-04-03 2023-04-11 [datetime.date(2023, 4, 12)]\n",
      "2023-04-03 2023-04-12 [datetime.date(2023, 4, 13)]\n",
      "2023-04-03 2023-04-13 [datetime.date(2023, 4, 14)]\n",
      "2023-04-03 2023-04-14 [datetime.date(2023, 4, 15)]\n",
      "2023-04-03 2023-04-15 [datetime.date(2023, 4, 16)]\n",
      "2023-04-03 2023-04-16 [datetime.date(2023, 4, 17)]\n",
      "2023-04-03 2023-04-17 [datetime.date(2023, 4, 18)]\n",
      "2023-04-03 2023-04-18 [datetime.date(2023, 4, 19)]\n",
      "2023-04-03 2023-04-19 [datetime.date(2023, 4, 20)]\n",
      "2023-04-03 2023-04-20 [datetime.date(2023, 4, 21)]\n",
      "2023-04-03 2023-04-21 [datetime.date(2023, 4, 22)]\n",
      "2023-04-03 2023-04-22 [datetime.date(2023, 4, 23)]\n"
     ]
    }
   ],
   "source": [
    "res_expanding = expanding_window_cv(df, 'rv_LT', 7, 'received_time_r', 1, XGBRegressor, params, freq, pair,\n",
    "                                    scaling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f35570fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanding_results = pd.DataFrame({'date': [i for i in res_expanding.keys()],\n",
    "                                'mae': [res_expanding[i]['mae'] for i, j in res_expanding.items()],\n",
    "                                'rmae': [res_expanding[i]['rmae'] for i, j in res_expanding.items()],\n",
    "                                'recall': [res_expanding[i]['recall'] for i, j in res_expanding.items()],\n",
    "                                'precision': [res_expanding[i]['precision'] for i, j in res_expanding.items()],\n",
    "                                'f1': [res_expanding[i]['f1'] for i, j in res_expanding.items()],\n",
    "                                'r2': [res_expanding[i]['r2'] for i, j in res_expanding.items()], \n",
    "                                'r2_rw': [res_expanding[i]['r2_rw'] for i, j in res_expanding.items()],\n",
    "                                'tstat_rw': [res_expanding[i]['tstat_rw'] for i, j in res_expanding.items()],\n",
    "                                'tstat_avg': [res_expanding[i]['tstat_avg'] for i, j in res_expanding.items()],\n",
    "                                'avg_zero_test': [res_expanding[i]['avg_zero_test'] for i, j in res_expanding.items()],\n",
    "                                'avg_zero_pred': [res_expanding[i]['avg_zero_pred'] for i, j in res_expanding.items()]} \n",
    "                               )\n",
    "\n",
    "expanding_results.to_excel(f'EW_{pair}_RESULTS_FREQ{freq}_BOOSTING.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "4fc3295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanding_features = pd.DataFrame()\n",
    "\n",
    "for i in res_sliding:\n",
    "    tmp_df = res_expanding[i]['fi']\n",
    "    tmp_df['date'] = i\n",
    "    \n",
    "    expanding_features = expanding_features.append(tmp_df)\n",
    "    \n",
    "expanding_features.to_excel(f'EW_FI_{pair}_freq{freq}.xlsx', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f84a04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'n_estimators': 500, \n",
    "          'max_depth': 6,  \n",
    "          'random_state': 81, \n",
    "          'n_jobs': 10\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "498bf5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e969a13bce4fd8acd9c1e96a5d8865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 2023-04-09 [datetime.date(2023, 4, 10)]\n",
      "2023-04-04 2023-04-10 [datetime.date(2023, 4, 11)]\n",
      "2023-04-05 2023-04-11 [datetime.date(2023, 4, 12)]\n",
      "2023-04-06 2023-04-12 [datetime.date(2023, 4, 13)]\n",
      "2023-04-07 2023-04-13 [datetime.date(2023, 4, 14)]\n",
      "2023-04-08 2023-04-14 [datetime.date(2023, 4, 15)]\n",
      "2023-04-09 2023-04-15 [datetime.date(2023, 4, 16)]\n",
      "2023-04-10 2023-04-16 [datetime.date(2023, 4, 17)]\n",
      "2023-04-11 2023-04-17 [datetime.date(2023, 4, 18)]\n",
      "2023-04-12 2023-04-18 [datetime.date(2023, 4, 19)]\n",
      "2023-04-13 2023-04-19 [datetime.date(2023, 4, 20)]\n",
      "2023-04-14 2023-04-20 [datetime.date(2023, 4, 21)]\n",
      "2023-04-15 2023-04-21 [datetime.date(2023, 4, 22)]\n",
      "2023-04-16 2023-04-22 [datetime.date(2023, 4, 23)]\n"
     ]
    }
   ],
   "source": [
    "res_sliding = sliding_window_cv(df, 'rv_LT', 7, 'received_time_r', 1, RandomForestRegressor, params, freq, pair,\n",
    "                                scaling = True)\n",
    "\n",
    "sliding_results = pd.DataFrame({'date': [i for i in res_sliding.keys()],\n",
    "                                'mae': [res_sliding[i]['mae'] for i, j in res_sliding.items()],\n",
    "                                'rmae': [res_sliding[i]['rmae'] for i, j in res_sliding.items()],\n",
    "                                'recall': [res_sliding[i]['recall'] for i, j in res_sliding.items()],\n",
    "                                'precision': [res_sliding[i]['precision'] for i, j in res_sliding.items()],\n",
    "                                'f1': [res_sliding[i]['f1'] for i, j in res_sliding.items()],\n",
    "                                'r2': [res_sliding[i]['r2'] for i, j in res_sliding.items()], \n",
    "                                'r2_rw': [res_sliding[i]['r2_rw'] for i, j in res_sliding.items()],\n",
    "                                'tstat_rw': [res_sliding[i]['tstat_rw'] for i, j in res_sliding.items()],\n",
    "                                'tstat_avg': [res_sliding[i]['tstat_avg'] for i, j in res_sliding.items()],\n",
    "                                'avg_zero_test': [res_sliding[i]['avg_zero_test'] for i, j in res_sliding.items()],\n",
    "                                'avg_zero_pred': [res_sliding[i]['avg_zero_pred'] for i, j in res_sliding.items()]} \n",
    "                               )\n",
    "\n",
    "sliding_results.to_excel(f'SW_{pair}_RESULTS_FREQ{freq}_RF.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "13b09193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 2023-04-09 [datetime.date(2023, 4, 10)]\n",
      "2023-04-03 2023-04-10 [datetime.date(2023, 4, 11)]\n",
      "2023-04-03 2023-04-11 [datetime.date(2023, 4, 12)]\n",
      "2023-04-03 2023-04-12 [datetime.date(2023, 4, 13)]\n",
      "2023-04-03 2023-04-13 [datetime.date(2023, 4, 14)]\n",
      "2023-04-03 2023-04-14 [datetime.date(2023, 4, 15)]\n",
      "2023-04-03 2023-04-15 [datetime.date(2023, 4, 16)]\n",
      "2023-04-03 2023-04-16 [datetime.date(2023, 4, 17)]\n",
      "2023-04-03 2023-04-17 [datetime.date(2023, 4, 18)]\n",
      "2023-04-03 2023-04-18 [datetime.date(2023, 4, 19)]\n",
      "2023-04-03 2023-04-19 [datetime.date(2023, 4, 20)]\n",
      "2023-04-03 2023-04-20 [datetime.date(2023, 4, 21)]\n",
      "2023-04-03 2023-04-21 [datetime.date(2023, 4, 22)]\n",
      "2023-04-03 2023-04-22 [datetime.date(2023, 4, 23)]\n"
     ]
    }
   ],
   "source": [
    "res_expanding = expanding_window_cv(df, 'rv_LT', 7, 'received_time_r', 1, RandomForestRegressor, params, freq, pair,\n",
    "                                    scaling = True)\n",
    "\n",
    "expanding_results = pd.DataFrame({'date': [i for i in res_expanding.keys()],\n",
    "                                'mae': [res_expanding[i]['mae'] for i, j in res_expanding.items()],\n",
    "                                'rmae': [res_expanding[i]['rmae'] for i, j in res_expanding.items()],\n",
    "                                'recall': [res_expanding[i]['recall'] for i, j in res_expanding.items()],\n",
    "                                'precision': [res_expanding[i]['precision'] for i, j in res_expanding.items()],\n",
    "                                'f1': [res_expanding[i]['f1'] for i, j in res_expanding.items()],\n",
    "                                'r2': [res_expanding[i]['r2'] for i, j in res_expanding.items()], \n",
    "                                'r2_rw': [res_expanding[i]['r2_rw'] for i, j in res_expanding.items()],\n",
    "                                'tstat_rw': [res_expanding[i]['tstat_rw'] for i, j in res_expanding.items()],\n",
    "                                'tstat_avg': [res_expanding[i]['tstat_avg'] for i, j in res_expanding.items()],\n",
    "                                'avg_zero_test': [res_expanding[i]['avg_zero_test'] for i, j in res_expanding.items()],\n",
    "                                'avg_zero_pred': [res_expanding[i]['avg_zero_pred'] for i, j in res_expanding.items()]} \n",
    "                               )\n",
    "\n",
    "expanding_results.to_excel(f'EW_{pair}_RESULTS_FREQ{freq}_RF.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7b5c7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_cv(data, target, min_sample, timestamp_col, window, algorythm, algo_params, freq, \n",
    "                      pair, scaling = False):\n",
    "    \n",
    "    date_col = 'date_col'\n",
    "    \n",
    "    data[date_col] = data[timestamp_col].apply(lambda x: x.date())\n",
    "    start_date, end_date = data[date_col].min(), data[date_col].max()\n",
    "    \n",
    "    delta = (end_date - start_date).days\n",
    "    n_iter = delta - window - min_sample + 1\n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    l = int(data.shape[0] * 0.1)\n",
    "    \n",
    "    for i in tqdm.tqdm_notebook(range(n_iter + 1)):\n",
    "        delta = relativedelta(days = +i)\n",
    "        start, end = start_date + delta, start_date + delta + relativedelta(days = +min_sample)\n",
    "        \n",
    "        train = data[(data[date_col] >= start) & (data[date_col] < end)]\n",
    "        test = data[(data[date_col] >= end) & (data[date_col] < end + relativedelta(days = +window))]\n",
    "        \n",
    "        print(train[date_col].min(), train[date_col].max(), test[date_col].unique())\n",
    "        \n",
    "        x_train, y_train = train.drop(columns = [target, timestamp_col, date_col]), train[target]\n",
    "        x_test, y_test = test.drop(columns = [target, timestamp_col, date_col]), test[target]\n",
    "        \n",
    "        if scaling:\n",
    "            sc = StandardScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "            x_test = sc.transform(x_test)\n",
    "            \n",
    "        model = algorythm(**algo_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        threshold = np.nanquantile(y_pred[:l], (y_train == 0).mean())\n",
    "\n",
    "        y_pred_bin = np.where(y_pred[l:] < threshold, 1, 0)\n",
    "        y_test_bin = np.where(y_test[l:] == 0, 1, 0)\n",
    "\n",
    "        recall = recall_score(y_test_bin, y_pred_bin)\n",
    "        precision = precision_score(y_test_bin, y_pred_bin)\n",
    "        f1 = f1_score(y_test_bin, y_pred_bin)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_rw = r2_score(y_test[:-1], y_test[1:])\n",
    "\n",
    "        mae_model = abs(y_pred - y_test)[1:]\n",
    "        mae_rw = abs(y_test[1:] - y_test[:-1])\n",
    "        mae_avg = abs(y_test - y_train.mean())\n",
    "\n",
    "        diff_rw = mae_rw - mae_model\n",
    "        tstat_rw = diff_rw.mean() / diff_rw.std() * np.sqrt(diff_rw.shape[0] - 1)\n",
    "\n",
    "        diff_avg = mae_avg - mae_model\n",
    "        tstat_avg = diff_avg.mean() / diff_avg.std() * np.sqrt(diff_avg.shape[0] - 1)\n",
    "        \n",
    "        results[end] = {'mae': mae(y_test, y_pred), 'rmae': rmae(y_test, y_pred),\n",
    "                        'recall': recall, 'precision': precision, 'f1': f1,\n",
    "                        'r2': r2, 'r2_rw': r2_rw, 'tstat_rw': tstat_rw, 'tstat_avg': tstat_avg,\n",
    "                        'avg_zero_test': y_test_bin.mean(), 'avg_zero_pred': y_pred_bin.mean()\n",
    "                        }\n",
    "        \n",
    "    return results\n",
    "\n",
    "def expanding_window_cv(data, target, min_sample, timestamp_col, window, algorythm, algo_params, freq,\n",
    "                        pair, scaling = False):\n",
    "    \n",
    "    date_col = 'date_col'\n",
    "    \n",
    "    data[date_col] = data[timestamp_col].apply(lambda x: x.date())\n",
    "    start_date, end_date = data[date_col].min(), data[date_col].max()\n",
    "    \n",
    "    delta = (end_date - start_date).days\n",
    "    n_iter = delta - window - min_sample + 1\n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    l = int(data.shape[0] * 0.1)\n",
    "    \n",
    "    for i in range(n_iter + 1):\n",
    "        delta = relativedelta(days = +i)\n",
    "        start, end = start_date, start_date + delta + relativedelta(days = +min_sample)\n",
    "        \n",
    "        train = data[(data[date_col] >= start) & (data[date_col] < end)]\n",
    "        test = data[(data[date_col] >= end) & (data[date_col] < end + relativedelta(days = +window))]\n",
    "        \n",
    "        print(train[date_col].min(), train[date_col].max(), test[date_col].unique())\n",
    "        \n",
    "        x_train, y_train = train.drop(columns = [target, timestamp_col, date_col]), train[target]\n",
    "        x_test, y_test = test.drop(columns = [target, timestamp_col, date_col]), test[target]\n",
    "        \n",
    "        if scaling:\n",
    "            sc = StandardScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "            x_test = sc.transform(x_test)\n",
    "            \n",
    "        model = algorythm(**algo_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        threshold = np.nanquantile(y_pred[:l], (y_train == 0).mean())\n",
    "\n",
    "        y_pred_bin = np.where(y_pred[l:] < threshold, 1, 0)\n",
    "        y_test_bin = np.where(y_test[l:] == 0, 1, 0)\n",
    "\n",
    "        recall = recall_score(y_test_bin, y_pred_bin)\n",
    "        precision = precision_score(y_test_bin, y_pred_bin)\n",
    "        f1 = f1_score(y_test_bin, y_pred_bin)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_rw = r2_score(y_test[:-1], y_test[1:])\n",
    "\n",
    "        mae_model = abs(y_pred - y_test)[1:]\n",
    "        mae_rw = abs(y_test[1:] - y_test[:-1])\n",
    "        mae_avg = abs(y_test - y_train.mean())\n",
    "\n",
    "        diff_rw = mae_rw - mae_model\n",
    "        tstat_rw = diff_rw.mean() / diff_rw.std() * np.sqrt(diff_rw.shape[0] - 1)\n",
    "\n",
    "        diff_avg = mae_avg - mae_model\n",
    "        tstat_avg = diff_avg.mean() / diff_avg.std() * np.sqrt(diff_avg.shape[0] - 1)\n",
    "        \n",
    "        results[end] = {'mae': mae(y_test, y_pred), 'rmae': rmae(y_test, y_pred),\n",
    "                        'recall': recall, 'precision': precision, 'f1': f1,\n",
    "                        'r2': r2, 'r2_rw': r2_rw, 'tstat_rw': tstat_rw, 'tstat_avg': tstat_avg,\n",
    "                        'avg_zero_test': y_test_bin.mean(), 'avg_zero_pred': y_pred_bin.mean()\n",
    "                       }\n",
    "        \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "91ade3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'alpha': 10\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ea35d280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732f3194c2f45bab575f6b4c00d6313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 2023-04-09 [datetime.date(2023, 4, 10)]\n",
      "2023-04-04 2023-04-10 [datetime.date(2023, 4, 11)]\n",
      "2023-04-05 2023-04-11 [datetime.date(2023, 4, 12)]\n",
      "2023-04-06 2023-04-12 [datetime.date(2023, 4, 13)]\n",
      "2023-04-07 2023-04-13 [datetime.date(2023, 4, 14)]\n",
      "2023-04-08 2023-04-14 [datetime.date(2023, 4, 15)]\n",
      "2023-04-09 2023-04-15 [datetime.date(2023, 4, 16)]\n",
      "2023-04-10 2023-04-16 [datetime.date(2023, 4, 17)]\n",
      "2023-04-11 2023-04-17 [datetime.date(2023, 4, 18)]\n",
      "2023-04-12 2023-04-18 [datetime.date(2023, 4, 19)]\n",
      "2023-04-13 2023-04-19 [datetime.date(2023, 4, 20)]\n",
      "2023-04-14 2023-04-20 [datetime.date(2023, 4, 21)]\n",
      "2023-04-15 2023-04-21 [datetime.date(2023, 4, 22)]\n",
      "2023-04-16 2023-04-22 [datetime.date(2023, 4, 23)]\n"
     ]
    }
   ],
   "source": [
    "res_sliding = sliding_window_cv(df, 'rv_LT', 7, 'received_time_r', 1, Lasso, params, freq, pair,\n",
    "                                scaling = True)\n",
    "\n",
    "sliding_results = pd.DataFrame({'date': [i for i in res_sliding.keys()],\n",
    "                                'mae': [res_sliding[i]['mae'] for i, j in res_sliding.items()],\n",
    "                                'rmae': [res_sliding[i]['rmae'] for i, j in res_sliding.items()],\n",
    "                                'recall': [res_sliding[i]['recall'] for i, j in res_sliding.items()],\n",
    "                                'precision': [res_sliding[i]['precision'] for i, j in res_sliding.items()],\n",
    "                                'f1': [res_sliding[i]['f1'] for i, j in res_sliding.items()],\n",
    "                                'r2': [res_sliding[i]['r2'] for i, j in res_sliding.items()], \n",
    "                                'r2_rw': [res_sliding[i]['r2_rw'] for i, j in res_sliding.items()],\n",
    "                                'tstat_rw': [res_sliding[i]['tstat_rw'] for i, j in res_sliding.items()],\n",
    "                                'tstat_avg': [res_sliding[i]['tstat_avg'] for i, j in res_sliding.items()],\n",
    "                                'avg_zero_test': [res_sliding[i]['avg_zero_test'] for i, j in res_sliding.items()],\n",
    "                                'avg_zero_pred': [res_sliding[i]['avg_zero_pred'] for i, j in res_sliding.items()]} \n",
    "                               )\n",
    "\n",
    "sliding_results.to_excel(f'SW_{pair}_RESULTS_FREQ{freq}_LASSO.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "43d7f4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 2023-04-09 [datetime.date(2023, 4, 10)]\n",
      "2023-04-03 2023-04-10 [datetime.date(2023, 4, 11)]\n",
      "2023-04-03 2023-04-11 [datetime.date(2023, 4, 12)]\n",
      "2023-04-03 2023-04-12 [datetime.date(2023, 4, 13)]\n",
      "2023-04-03 2023-04-13 [datetime.date(2023, 4, 14)]\n",
      "2023-04-03 2023-04-14 [datetime.date(2023, 4, 15)]\n",
      "2023-04-03 2023-04-15 [datetime.date(2023, 4, 16)]\n",
      "2023-04-03 2023-04-16 [datetime.date(2023, 4, 17)]\n",
      "2023-04-03 2023-04-17 [datetime.date(2023, 4, 18)]\n",
      "2023-04-03 2023-04-18 [datetime.date(2023, 4, 19)]\n",
      "2023-04-03 2023-04-19 [datetime.date(2023, 4, 20)]\n",
      "2023-04-03 2023-04-20 [datetime.date(2023, 4, 21)]\n",
      "2023-04-03 2023-04-21 [datetime.date(2023, 4, 22)]\n",
      "2023-04-03 2023-04-22 [datetime.date(2023, 4, 23)]\n"
     ]
    }
   ],
   "source": [
    "res_expanding = expanding_window_cv(df, 'rv_LT', 7, 'received_time_r', 1, Lasso, params, freq, pair,\n",
    "                                    scaling = True)\n",
    "\n",
    "expanding_results = pd.DataFrame({'date': [i for i in res_expanding.keys()],\n",
    "                                'mae': [res_expanding[i]['mae'] for i, j in res_expanding.items()],\n",
    "                                'rmae': [res_expanding[i]['rmae'] for i, j in res_expanding.items()],\n",
    "                                'recall': [res_expanding[i]['recall'] for i, j in res_expanding.items()],\n",
    "                                'precision': [res_expanding[i]['precision'] for i, j in res_expanding.items()],\n",
    "                                'f1': [res_expanding[i]['f1'] for i, j in res_expanding.items()],\n",
    "                                'r2': [res_expanding[i]['r2'] for i, j in res_expanding.items()], \n",
    "                                'r2_rw': [res_expanding[i]['r2_rw'] for i, j in res_expanding.items()],\n",
    "                                'tstat_rw': [res_expanding[i]['tstat_rw'] for i, j in res_expanding.items()],\n",
    "                                'tstat_avg': [res_expanding[i]['tstat_avg'] for i, j in res_expanding.items()],\n",
    "                                'avg_zero_test': [res_expanding[i]['avg_zero_test'] for i, j in res_expanding.items()],\n",
    "                                'avg_zero_pred': [res_expanding[i]['avg_zero_pred'] for i, j in res_expanding.items()]} \n",
    "                               )\n",
    "\n",
    "expanding_results.to_excel(f'EW_{pair}_RESULTS_FREQ{freq}_LASSO.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d0fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
